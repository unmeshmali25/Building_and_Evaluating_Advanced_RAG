{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/c536898/Documents/Engineering/AI/Building and Evaluating Advanced RAG/basic_RAG.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils\u001b[39;00m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Engineering/AI/Building and Evaluating Advanced RAG/utils.py:32\u001b[0m\n\u001b[1;32m     28\u001b[0m     _ \u001b[39m=\u001b[39m load_dotenv(find_dotenv())\n\u001b[1;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mHUGGINGFACE_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m openai \u001b[39m=\u001b[39m OpenAI()\n\u001b[1;32m     34\u001b[0m qa_relevance \u001b[39m=\u001b[39m (\n\u001b[1;32m     35\u001b[0m     Feedback(openai\u001b[39m.\u001b[39mrelevance_with_cot_reasons, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAnswer Relevance\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m     \u001b[39m.\u001b[39mon_input_output()\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m qs_relevance \u001b[39m=\u001b[39m (\n\u001b[1;32m     40\u001b[0m     Feedback(openai\u001b[39m.\u001b[39mrelevance_with_cot_reasons, name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mContext Relevance\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[39m.\u001b[39mon_input()\n\u001b[1;32m     42\u001b[0m     \u001b[39m.\u001b[39mon(TruLlama\u001b[39m.\u001b[39mselect_source_nodes()\u001b[39m.\u001b[39mnode\u001b[39m.\u001b[39mtext)\n\u001b[1;32m     43\u001b[0m     \u001b[39m.\u001b[39maggregate(np\u001b[39m.\u001b[39mmean)\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/trulens_eval/feedback/provider/openai.py:62\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, endpoint, model_engine, *args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m self_kwargs\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     60\u001b[0m self_kwargs[\u001b[39m'\u001b[39m\u001b[39mmodel_engine\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_engine\n\u001b[0;32m---> 62\u001b[0m self_kwargs[\u001b[39m'\u001b[39m\u001b[39mendpoint\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m OpenAIEndpoint(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mself_kwargs\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/trulens_eval/feedback/provider/endpoint/openai.py:243\u001b[0m, in \u001b[0;36mOpenAIEndpoint.__init__\u001b[0;34m(self, rpm, name, client, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[CLASS_INFO]\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m client \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     \u001b[39m# Pass kwargs to client.\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     client \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mOpenAI(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    244\u001b[0m     self_kwargs[\u001b[39m'\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m OpenAIClient(client\u001b[39m=\u001b[39mclient)\n\u001b[1;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_client.py:98\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     96\u001b[0m     api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     99\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m api_key\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m organization \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import utils \n",
    "import os\n",
    "import openai\n",
    "openai.api_key = utils.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files = [\"./unmeshmaliWeeklyBlog.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "document = Document(text = \"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "43 \n",
      "\n",
      "<class 'llama_index.schema.Document'> \n",
      "\n",
      "Doc ID: 4ee6cb87-3dd2-417d-a3ef-80919f6fa4ef\n",
      "Text: unmeshmali.com Posts published on February 18, 2024, from\n",
      "unmeshmali.com. Printed on February 18, 2024 using Print My Blog Week\n",
      "10, 2021 March 11, 2021 Categories: Weekly Notes Work Working on\n",
      "multiple things like always. I have a couple technical projects in\n",
      "hand that involve Python and Java programming. I also learned Docker\n",
      "to be able to depl...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]), \"\\n\")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window-sentence retrieval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# creating note parser with default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_metadata_key =\"window\", \n",
    "    window_size=2,\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi. This is Unmesh. I am in week 7 of 2024. This has been a busy year at work so far. I want to make this year extremely productive.\"\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([Document(text = text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi. ', 'This is Unmesh. ', 'I am in week 7 of 2024. ', 'This has been a busy year at work so far. ', 'I want to make this year extremely productive.']\n"
     ]
    }
   ],
   "source": [
    "print([x.text for x in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am in week 7 of 2024.  This has been a busy year at work so far.  I want to make this year extremely productive.\n"
     ]
    }
   ],
   "source": [
    "# Checking metadata around some nodes (each node is a sentence)\n",
    "print(nodes[4].metadata[\"window\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "# Createing the GPT 3.5 model object\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "sentence_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    node_parser=node_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "sentence_index = VectorStoreIndex.from_documents(\n",
    "    [document], service_context=sentence_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the post-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "postproc =  MetadataReplacementPostProcessor(\n",
    "    target_metadata_key=\"window\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import NodeWithScore\n",
    "from copy import deepcopy\n",
    "\n",
    "scored_nodes = [NodeWithScore(node=x, score=1.0) for x in nodes]\n",
    "nodes_old = [deepcopy(n) for n in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is Unmesh. '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_old[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_nodes = postproc.postprocess_nodes(scored_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi.  This is Unmesh.  I am in week 7 of 2024. \n"
     ]
    }
   ],
   "source": [
    "print(replaced_nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 799/799 [00:00<00:00, 565kB/s]\n",
      "model.safetensors: 100%|██████████| 1.11G/1.11G [00:24<00:00, 44.7MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 443/443 [00:00<00:00, 491kB/s]\n",
      "sentencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 36.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.1M/17.1M [00:00<00:00, 41.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 279/279 [00:00<00:00, 405kB/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    top_n=2, \n",
    "    model = \"BAAI/bge-reranker-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.schema import TextNode, NodeWithScore\n",
    "\n",
    "query = QueryBundle(\"I want a dog\")\n",
    "\n",
    "scored_nodes = [\n",
    "    NodeWithScore(node=TextNode(text = \"This is a cat\"), score = 0.3), \n",
    "    NodeWithScore(node=TextNode(text=\"This is a dog\"), score = 0.4)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reranked_nodes = rerank.postprocess_nodes(\n",
    "    scored_nodes, query_bundle=query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print([(x.text, x.score) for x in reranked_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=6, node_postprocessors=[postproc, rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot reuse already awaited coroutine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/c536898/Documents/Engineering/AI/Building and Evaluating Advanced RAG/basic_RAG.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m window_response \u001b[39m=\u001b[39m sentence_window_engine\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mWhere was Unmesh employed in 2022\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/core/base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     39\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/query_engine/retriever_query_engine.py:172\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    169\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    170\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[1;32m    171\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 172\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    173\u001b[0m         query\u001b[39m=\u001b[39;49mquery_bundle,\n\u001b[1;32m    174\u001b[0m         nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    177\u001b[0m     query_event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    179\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/base.py:168\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    165\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    166\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    167\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 168\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    169\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    170\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    171\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    172\u001b[0m         ],\n\u001b[1;32m    173\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    177\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m new_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m     39\u001b[0m     query_str\u001b[39m=\u001b[39;49mquery_str,\n\u001b[1;32m     40\u001b[0m     text_chunks\u001b[39m=\u001b[39;49mnew_texts,\n\u001b[1;32m     41\u001b[0m     prev_response\u001b[39m=\u001b[39;49mprev_response,\n\u001b[1;32m     42\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/refine.py:146\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m prev_response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[39m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_single(\n\u001b[1;32m    147\u001b[0m             query_str, text_chunk, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs\n\u001b[1;32m    148\u001b[0m         )\n\u001b[1;32m    149\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[39m# refine response if possible\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refine_response_single(\n\u001b[1;32m    152\u001b[0m             prev_response, query_str, text_chunk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kwargs\n\u001b[1;32m    153\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/refine.py:202\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m         structured_response \u001b[39m=\u001b[39m cast(\n\u001b[1;32m    201\u001b[0m             StructuredRefineResponse,\n\u001b[0;32m--> 202\u001b[0m             program(\n\u001b[1;32m    203\u001b[0m                 context_str\u001b[39m=\u001b[39;49mcur_text_chunk,\n\u001b[1;32m    204\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    205\u001b[0m             ),\n\u001b[1;32m    206\u001b[0m         )\n\u001b[1;32m    207\u001b[0m         query_satisfied \u001b[39m=\u001b[39m structured_response\u001b[39m.\u001b[39mquery_satisfied\n\u001b[1;32m    208\u001b[0m         \u001b[39mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/refine.py:64\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     62\u001b[0m     answer \u001b[39m=\u001b[39m answer\u001b[39m.\u001b[39mjson()\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     65\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt,\n\u001b[1;32m     66\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[39m=\u001b[39manswer, query_satisfied\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llms/llm.py:239\u001b[0m, in \u001b[0;36mLLM.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mis_chat_model:\n\u001b[1;32m    238\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_messages(prompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[0;32m--> 239\u001b[0m     chat_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat(messages)\n\u001b[1;32m    240\u001b[0m     output \u001b[39m=\u001b[39m chat_response\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llms/base.py:100\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m     92\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m     93\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m     94\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         },\n\u001b[1;32m     99\u001b[0m     )\n\u001b[0;32m--> 100\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    103\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llms/openai.py:237\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     chat_fn \u001b[39m=\u001b[39m completion_to_chat_decorator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_complete)\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m chat_fn(messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llms/openai.py:296\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_client()\n\u001b[1;32m    295\u001b[0m message_dicts \u001b[39m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 296\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    297\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts,\n\u001b[1;32m    298\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    299\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_model_kwargs(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    300\u001b[0m )\n\u001b[1;32m    301\u001b[0m openai_message \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\n\u001b[1;32m    302\u001b[0m message \u001b[39m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/trulens_eval/feedback/provider/endpoint/base.py:685\u001b[0m, in \u001b[0;36mEndpoint.wrap_function.<locals>.tru_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtru_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    680\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    681\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCalling instrumented sync method \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(func)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39miscoroutinefunction=\u001b[39m\u001b[39m{\u001b[39;00mis_really_coroutinefunction(func)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39misasyncgeneratorfunction=\u001b[39m\u001b[39m{\u001b[39;00minspect\u001b[39m.\u001b[39misasyncgenfunction(func)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[0;32m--> 685\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(tru_awrapper, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/trulens_eval/utils/asynchro.py:164\u001b[0m, in \u001b[0;36msync\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m# Get the return or error, return the return or raise the error.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m thread\u001b[39m.\u001b[39merror \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mraise\u001b[39;00m thread\u001b[39m.\u001b[39merror\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m thread\u001b[39m.\u001b[39mret\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/trulens_eval/utils/asynchro.py:152\u001b[0m, in \u001b[0;36msync.<locals>.run_in_new_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     loop \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mnew_event_loop()\n\u001b[0;32m--> 152\u001b[0m     th\u001b[39m.\u001b[39mret \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39;49mrun_until_complete(awaitable)\n\u001b[1;32m    153\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     th\u001b[39m.\u001b[39merror \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_traceback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    257\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot reuse already awaited coroutine"
     ]
    }
   ],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"Where was Unmesh employed in 2022\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'window_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/c536898/Documents/Engineering/AI/Building and Evaluating Advanced RAG/basic_RAG.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnotebook_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m display_response\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m display_response(window_response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'window_response' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
