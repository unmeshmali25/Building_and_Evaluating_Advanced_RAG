{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "import utils \n",
    "import os\n",
    "import openai\n",
    "openai.api_key = utils.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files = [\"./unmeshmaliWeeklyBlog.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "document = Document(text = \"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "43 \n",
      "\n",
      "<class 'llama_index.schema.Document'> \n",
      "\n",
      "Doc ID: 16b44abe-b8f3-4309-9bed-fa5e5a6f4622\n",
      "Text: unmeshmali.com Posts published on February 18, 2024, from\n",
      "unmeshmali.com. Printed on February 18, 2024 using Print My Blog Week\n",
      "10, 2021 March 11, 2021 Categories: Weekly Notes Work Working on\n",
      "multiple things like always. I have a couple technical projects in\n",
      "hand that involve Python and Java programming. I also learned Docker\n",
      "to be able to depl...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]), \"\\n\")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window-sentence retrieval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# creating note parser with default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_metadata_key =\"window\", \n",
    "    window_size=2,\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi. This is Unmesh. I am in week 7 of 2024. This has been a busy year at work so far. I want to make this year extremely productive.\"\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([Document(text = text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi. ', 'This is Unmesh. ', 'I am in week 7 of 2024. ', 'This has been a busy year at work so far. ', 'I want to make this year extremely productive.']\n"
     ]
    }
   ],
   "source": [
    "print([x.text for x in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am in week 7 of 2024.  This has been a busy year at work so far.  I want to make this year extremely productive.\n"
     ]
    }
   ],
   "source": [
    "# Checking metadata around some nodes (each node is a sentence)\n",
    "print(nodes[4].metadata[\"window\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "# Createing the GPT 3.5 model object\n",
    "llm = OpenAI(model=\"gpt-3.5\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/c536898/Documents/Engineering/AI/Building and Evaluating Advanced RAG/basic_RAG.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m \u001b[39mimport\u001b[39;00m ServiceContext\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sentence_context \u001b[39m=\u001b[39m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     llm\u001b[39m=\u001b[39;49mllm,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     embed_model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlocal:BAAI/bge-small-en-v1.5\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     node_parser\u001b[39m=\u001b[39;49mnode_parser,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/service_context.py:191\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    186\u001b[0m         llm_predictor\u001b[39m.\u001b[39mquery_wrapper_prompt \u001b[39m=\u001b[39m query_wrapper_prompt\n\u001b[1;32m    188\u001b[0m \u001b[39m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m# NOTE: embed model should be a transformation, but the way the service\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39m# context works, we can't put in there yet.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m embed_model \u001b[39m=\u001b[39m resolve_embed_model(embed_model)\n\u001b[1;32m    192\u001b[0m embed_model\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n\u001b[1;32m    194\u001b[0m prompt_helper \u001b[39m=\u001b[39m prompt_helper \u001b[39mor\u001b[39;00m _get_default_prompt_helper(\n\u001b[1;32m    195\u001b[0m     llm_metadata\u001b[39m=\u001b[39mllm_predictor\u001b[39m.\u001b[39mmetadata,\n\u001b[1;32m    196\u001b[0m     context_window\u001b[39m=\u001b[39mcontext_window,\n\u001b[1;32m    197\u001b[0m     num_output\u001b[39m=\u001b[39mnum_output,\n\u001b[1;32m    198\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/embeddings/utils.py:84\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model)\u001b[0m\n\u001b[1;32m     80\u001b[0m         embed_model \u001b[39m=\u001b[39m InstructorEmbedding(\n\u001b[1;32m     81\u001b[0m             model_name\u001b[39m=\u001b[39mmodel_name, cache_folder\u001b[39m=\u001b[39mcache_folder\n\u001b[1;32m     82\u001b[0m         )\n\u001b[1;32m     83\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m         embed_model \u001b[39m=\u001b[39m HuggingFaceEmbedding(\n\u001b[1;32m     85\u001b[0m             model_name\u001b[39m=\u001b[39;49mmodel_name, cache_folder\u001b[39m=\u001b[39;49mcache_folder\n\u001b[1;32m     86\u001b[0m         )\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m LCEmbeddings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(embed_model, LCEmbeddings):\n\u001b[1;32m     89\u001b[0m     embed_model \u001b[39m=\u001b[39m LangchainEmbedding(embed_model)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/embeddings/huggingface.py:83\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding.__init__\u001b[0;34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# Use model_name with AutoModel\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     model_name \u001b[39m=\u001b[39m (\n\u001b[1;32m     79\u001b[0m         model_name\n\u001b[1;32m     80\u001b[0m         \u001b[39mif\u001b[39;00m model_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         \u001b[39melse\u001b[39;00m DEFAULT_HUGGINGFACE_EMBEDDING_MODEL\n\u001b[1;32m     82\u001b[0m     )\n\u001b[0;32m---> 83\u001b[0m     model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     84\u001b[0m         model_name, cache_dir\u001b[39m=\u001b[39mcache_folder, trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[39melif\u001b[39;00m model_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# Extract model_name from model\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     model_name \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mname_or_path\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1304\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1303\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1304\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1292\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1290\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m-> 1292\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "sentence_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    node_parser=node_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
