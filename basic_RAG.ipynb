{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import openai\n",
    "with open('./openkey.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = data['openai-api-key']\n",
    "openai.api_key = utils.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./unmeshmaliWeeklyBlog.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> /n\n",
      "Doc ID: 5532a407-083a-4004-a39f-65879a143f3b\n",
      "Text: unmeshmali.com Posts published on February 18, 2024, from\n",
      "unmeshmali.com. Printed on February 18, 2024 using Print My Blog Week\n",
      "10, 2021 March 11, 2021 Categories: Weekly Notes Work Working on\n",
      "multiple things like always. I have a couple technical projects in\n",
      "hand that involve Python and Java programming. I also learned Docker\n",
      "to be able to depl...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"/n\")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo\", temperature=0.1\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "\n",
    "index = VectorStoreIndex.from_documents([document], \n",
    "                                        service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_um = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot reuse already awaited coroutine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/c536898/Documents/Engineering/AI/Building and Evaluating Advanced RAG/basic_RAG.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m query_engine_um\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mWhere was Unmesh employed in May 2022\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#Y103sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#Y103sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mstr\u001b[39m(response))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/core/base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     39\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/query_engine/retriever_query_engine.py:172\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    169\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    170\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[1;32m    171\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 172\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    173\u001b[0m         query\u001b[39m=\u001b[39;49mquery_bundle,\n\u001b[1;32m    174\u001b[0m         nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    177\u001b[0m     query_event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    179\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/base.py:168\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    165\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    166\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    167\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 168\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    169\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    170\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    171\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    172\u001b[0m         ],\n\u001b[1;32m    173\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    177\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m new_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m     39\u001b[0m     query_str\u001b[39m=\u001b[39;49mquery_str,\n\u001b[1;32m     40\u001b[0m     text_chunks\u001b[39m=\u001b[39;49mnew_texts,\n\u001b[1;32m     41\u001b[0m     prev_response\u001b[39m=\u001b[39;49mprev_response,\n\u001b[1;32m     42\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/refine.py:146\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m prev_response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[39m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_single(\n\u001b[1;32m    147\u001b[0m             query_str, text_chunk, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs\n\u001b[1;32m    148\u001b[0m         )\n\u001b[1;32m    149\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[39m# refine response if possible\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refine_response_single(\n\u001b[1;32m    152\u001b[0m             prev_response, query_str, text_chunk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kwargs\n\u001b[1;32m    153\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/refine.py:202\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m         structured_response \u001b[39m=\u001b[39m cast(\n\u001b[1;32m    201\u001b[0m             StructuredRefineResponse,\n\u001b[0;32m--> 202\u001b[0m             program(\n\u001b[1;32m    203\u001b[0m                 context_str\u001b[39m=\u001b[39;49mcur_text_chunk,\n\u001b[1;32m    204\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    205\u001b[0m             ),\n\u001b[1;32m    206\u001b[0m         )\n\u001b[1;32m    207\u001b[0m         query_satisfied \u001b[39m=\u001b[39m structured_response\u001b[39m.\u001b[39mquery_satisfied\n\u001b[1;32m    208\u001b[0m         \u001b[39mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/response_synthesizers/refine.py:64\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     62\u001b[0m     answer \u001b[39m=\u001b[39m answer\u001b[39m.\u001b[39mjson()\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     65\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt,\n\u001b[1;32m     66\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[39m=\u001b[39manswer, query_satisfied\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llms/llm.py:239\u001b[0m, in \u001b[0;36mLLM.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mis_chat_model:\n\u001b[1;32m    238\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_messages(prompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[0;32m--> 239\u001b[0m     chat_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat(messages)\n\u001b[1;32m    240\u001b[0m     output \u001b[39m=\u001b[39m chat_response\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llms/base.py:100\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m     92\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m     93\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m     94\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         },\n\u001b[1;32m     99\u001b[0m     )\n\u001b[0;32m--> 100\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    103\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llms/openai.py:237\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     chat_fn \u001b[39m=\u001b[39m completion_to_chat_decorator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_complete)\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m chat_fn(messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llms/openai.py:296\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_client()\n\u001b[1;32m    295\u001b[0m message_dicts \u001b[39m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 296\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    297\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts,\n\u001b[1;32m    298\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    299\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_model_kwargs(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    300\u001b[0m )\n\u001b[1;32m    301\u001b[0m openai_message \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\n\u001b[1;32m    302\u001b[0m message \u001b[39m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/trulens_eval/feedback/provider/endpoint/base.py:685\u001b[0m, in \u001b[0;36mEndpoint.wrap_function.<locals>.tru_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtru_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    680\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    681\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCalling instrumented sync method \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(func)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39miscoroutinefunction=\u001b[39m\u001b[39m{\u001b[39;00mis_really_coroutinefunction(func)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39misasyncgeneratorfunction=\u001b[39m\u001b[39m{\u001b[39;00minspect\u001b[39m.\u001b[39misasyncgenfunction(func)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[0;32m--> 685\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(tru_awrapper, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/trulens_eval/utils/asynchro.py:164\u001b[0m, in \u001b[0;36msync\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m# Get the return or error, return the return or raise the error.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m thread\u001b[39m.\u001b[39merror \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mraise\u001b[39;00m thread\u001b[39m.\u001b[39merror\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m thread\u001b[39m.\u001b[39mret\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/trulens_eval/utils/asynchro.py:152\u001b[0m, in \u001b[0;36msync.<locals>.run_in_new_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     loop \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mnew_event_loop()\n\u001b[0;32m--> 152\u001b[0m     th\u001b[39m.\u001b[39mret \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39;49mrun_until_complete(awaitable)\n\u001b[1;32m    153\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     th\u001b[39m.\u001b[39merror \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_traceback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    257\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot reuse already awaited coroutine"
     ]
    }
   ],
   "source": [
    "response = query_engine_um.query(\n",
    "    \"Where was Unmesh employed in May 2022\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure how to solve the above error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot reuse already awaited coroutine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/c536898/Documents/Engineering/AI/Building and Evaluating Advanced RAG/basic_RAG.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#Y104sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m instance \u001b[39m=\u001b[39m foo()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mawait\u001b[39;00m instance)  \u001b[39m# Okay the first time\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/c536898/Documents/Engineering/AI/Building%20and%20Evaluating%20Advanced%20RAG/basic_RAG.ipynb#Y104sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mawait\u001b[39;00m instance)  \u001b[39m# Second time should be an error\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot reuse already awaited coroutine"
     ]
    }
   ],
   "source": [
    "# Trying to replicate the above error \n",
    "\n",
    "async def foo():\n",
    "    return 123\n",
    "\n",
    "\n",
    "instance = foo()\n",
    "print(await instance)  # Okay the first time\n",
    "print(await instance)  # Second time should be an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "async def foo():\n",
    "    return 123\n",
    "\n",
    "# Create and await a new instance for the first time\n",
    "instance1 = foo()\n",
    "print(await instance1)\n",
    "\n",
    "# Create and await a new instance for the second time\n",
    "instance2 = foo()\n",
    "print(await instance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
